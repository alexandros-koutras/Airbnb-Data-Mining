{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLiXtpLHAZkL"
      },
      "source": [
        "# **Εργασία** **2** **-** **Τεχνικές** **Εξόρυξης** **Δεδομένων**\n",
        "Ονοματεπώνυμα: Αναστασία Ορφανουδάκη & Αλέξανδρος Κούτρας\n",
        "\n",
        "Α.Μ: 1115202100129 & 1115202100073\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD3nwkLmAf2L"
      },
      "source": [
        "# **ΜΕΡΟΣ 1ο**\n",
        "\n",
        "Φορτώνουμε τη στήλη comments για κάθε μήνα του 2019, τα ενώνουμε σε ένα dataframe και τα προεπεξεργαζόματσε, αφαιρώντας τα σημεία\n",
        "στίξης, μετατρέποντας όλους τους χαρακτήρες σε μικρούς, αφαιρώντας τα σύμβολα,\n",
        "όπως hashtags, emoticons,emojis, links καθώς και τα stopwords από το σύνολο των\n",
        "δεδομενων."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "XWgD0ZxyBEar"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "WmiMPgjTD60J"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "CSUCf_0yhI4s"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "JBCVPwQNGlWd"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "j2vEYhC7ra6_"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "JVs8kzD2zaZ1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "TN40K7uiNmn6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "hSB374uihFuq"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "d9S-OQh8hMlV"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "UUOh_1OCPPxZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "kHntJEDHhVTF"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score, cross_val_predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "ZPLnpazahW7y"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "yBzSZ0BOhYl-"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "4_ngQQ7zha0d"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "NU0QVMeBhBUJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api"
      ],
      "metadata": {
        "id": "j4W6Ryo-yveN"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "FSysnORnyxaE"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYDkDZAr5GHa",
        "outputId": "84a6d177-b89c-40e3-971c-b0e5f49c7bf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "MvMpI5t1nwvo"
      },
      "outputs": [],
      "source": [
        "from langdetect import detect, LangDetectException"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUrG0o72ISZ0",
        "outputId": "fcaf4885-6685-4f35-af21-eb721dbec425"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGQRVCkKBLiZ",
        "outputId": "5162294e-310d-40cc-bc32-2151af03a564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "BgqY96VmAODU"
      },
      "outputs": [],
      "source": [
        "# Διαβάζουμε τα αρχεία reviews.csv για να πάρουμε την στήλη σχόλιαγια τους μήνες Φεβρπυάριο, Μάρτιο και Απρίλιο\n",
        "commentsFeb19 = pd.read_csv('/content/gdrive/MyDrive/data/data/2019/febrouary/reviews.csv')\n",
        "commentsMarch19 = pd.read_csv('/content/gdrive/MyDrive/data/data/2019/march/reviews.csv')\n",
        "commentsApril19 = pd.read_csv('/content/gdrive/MyDrive/data/data/2019/april/reviews.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "gvfBvI3aB9zn"
      },
      "outputs": [],
      "source": [
        "# Προσθέτουμε στήλη με τον μήνα για κάθε σετ δεδομένων\n",
        "commentsFeb19['month'] = '02'\n",
        "commentsMarch19['month'] = '03'\n",
        "commentsApril19['month'] = '04'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "cR20wQiCCkEK"
      },
      "outputs": [],
      "source": [
        "# Ενώνουμε τα αρχεία\n",
        "comments_2019 = pd.concat([commentsFeb19, commentsMarch19, commentsApril19], ignore_index=False)\n",
        "\n",
        "# Αντικαθιστούμε τις NaN τιμές με 0\n",
        "comments_2019.fillna(0, inplace=True )\n",
        "\n",
        "# Αποθηκεύουμε το αρχείο σε csv\n",
        "comments_2019.to_csv('comments_2019.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfFMOe2wGr-k"
      },
      "source": [
        "Η ίδια διαδικασία για το 2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "i5ww6HwSG6Ph"
      },
      "outputs": [],
      "source": [
        "# Διαβάζουμε τα αρχεία reviews.csv για να πάρουμε την στήλη σχόλιαγια τους μήνες Μάρτιο, Ιούνιο, Σεπτέμβριο\n",
        "commentsMarch23 = pd.read_csv('/content/gdrive/MyDrive/data/data/2023/march/reviews.csv')\n",
        "commentsJune23 = pd.read_csv('/content/gdrive/MyDrive/data/data/2023/june/reviews.csv')\n",
        "commentsSept23 = pd.read_csv('/content/gdrive/MyDrive/data/data/2023/september/reviews.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "SE9N_TyxG44z"
      },
      "outputs": [],
      "source": [
        "# Προσθέτουμε στήλη με τον μήνα για κάθε σετ δεδομένων\n",
        "commentsMarch23['month'] = '03'\n",
        "commentsJune23['month'] = '06'\n",
        "commentsSept23['month'] = '09'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "v0SZhT8sG3mR"
      },
      "outputs": [],
      "source": [
        "# Ενώνουμε τα αρχεία\n",
        "comments_2023 = pd.concat([commentsMarch23, commentsJune23, commentsSept23], ignore_index=False)\n",
        "\n",
        "# Αντικαθιστούμε τις NaN τιμές με 0\n",
        "comments_2023.fillna(0, inplace=True )\n",
        "\n",
        "# Αποθηκεύουμε το αρχείο σε csv\n",
        "comments_2023.to_csv('comments_2023.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkCBOGYAIlLZ"
      },
      "source": [
        "## **1o bullet**\n",
        "\n",
        "Χρησιμοποιώντας το HuggingFace επισημείωστε (annotation process) ως\n",
        "προς το συναίσθημα (θετικό/αρνητικό/ουδέτερο) όσα περισσότερα comments\n",
        "μπορείτε για το 2019. Στο τελικό αποτέλεσμα θα χρειαστεί να έχετε και ένα αναγνωριστικό id. Δηλαδή θα προκύψει ένα csv αρχείο (ή ένα dataframe) που θα έχει 3 στήλες (id, review,\n",
        "sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WZXbSSGnF8XF",
        "outputId": "2c04dd53-11c7-446d-8a6c-09743873fc25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Παίρνουμε ένα ποσοστό από τα αρχεία\n",
        "portion19 = comments_2019.sample(frac=0.01, random_state=42)\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "def clean_text(text, chunk_size=500):\n",
        "    # Διατηρούμε μόνο γράμματα και κενά και τα μετατρέπουμε σε μικρούς χαρακτήρες\n",
        "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', str(text))\n",
        "    cleaned_text = cleaned_text.lower()\n",
        "    # Χρησιμοποιούμε την regex για να αντικαταστήσετε τους ειδικούς χαρακτήρες με κενό string\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "    # Αφαιρούμε τα stopwords\n",
        "    words = cleaned_text.split()\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    # Ενώνουμε τις φιλτραρισμένες λέξεις πίσω σε ένα string\n",
        "    cleaned_text = ' '.join(filtered_words)\n",
        "    chunked_text = [cleaned_text[i:i+chunk_size] for i in range(0, len(cleaned_text), chunk_size)]\n",
        "    return chunked_text\n",
        "\n",
        "def classify_sentiment(chunks):\n",
        "    total_sentiment = 0\n",
        "\n",
        "    for chunk in chunks:\n",
        "        result = classifier(chunk)\n",
        "        if result[0]['label'] == 'POSITIVE':\n",
        "            total_sentiment += 1\n",
        "        elif result[0]['label'] == 'NEGATIVE':\n",
        "            total_sentiment -= 1\n",
        "\n",
        "    # Καθορίζει το ολικό συναίσθημα με βάση το total_sentiment\n",
        "    if total_sentiment > 0:\n",
        "        return'POSITIVE'\n",
        "    elif total_sentiment < 0:\n",
        "        return 'NEGATIVE'\n",
        "    else:\n",
        "        return 'NEUTRAL'\n",
        "\n",
        "    return total_sentiment\n",
        "\n",
        "\n",
        "# Αφαιρούμε τα μη Αγγλικά σχόλια\n",
        "def is_english(text):\n",
        "    try:\n",
        "        return detect(str(text)) == 'en'\n",
        "    except LangDetectException:\n",
        "        return False\n",
        "\n",
        "portion19 = portion19[portion19['comments'].apply(is_english)]\n",
        "\n",
        "portion19['review'] = portion19['comments'].apply(clean_text)\n",
        "portion19['sentiment'] = portion19['review'].apply(classify_sentiment)\n",
        "\n",
        "portion19['review'] = portion19['review'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "portion19 = portion19.loc[:, ['listing_id', 'review', 'sentiment', 'month']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsN9bwTgDZwk"
      },
      "source": [
        "**2ο bullet**\n",
        "\n",
        "Κάντε το ίδιο για τις κριτικές του 2023.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRGTni_GUzyw",
        "outputId": "e4fbc82e-b5a1-4eb7-ab2a-c1a5674bca57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "portion23 = comments_2023.sample(frac=0.01, random_state=42)\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "def clean_text(text, chunk_size=500):\n",
        "    # Διατηρούμε μόνο γράμματα και κενά και τα μετατρέπουμε σε μικρούς χαρακτήρες\n",
        "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', str(text))\n",
        "    cleaned_text = cleaned_text.lower()\n",
        "    # Χρησιμοποιούμε την regex για να αντικαταστήσετε τους ειδικούς χαρακτήρες με κενό string\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "    # Αφαιρούμε τα stopwords\n",
        "    words = cleaned_text.split()\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    # Ενώνουμε τις φιλτραρισμένες λέξεις πίσω σε ένα string\n",
        "    cleaned_text = ' '.join(filtered_words)\n",
        "    chunked_text = [cleaned_text[i:i+chunk_size] for i in range(0, len(cleaned_text), chunk_size)]\n",
        "    return chunked_text\n",
        "\n",
        "def classify_sentiment(chunks):\n",
        "    total_sentiment = 0\n",
        "\n",
        "    for chunk in chunks:\n",
        "        result = classifier(chunk)\n",
        "        if result[0]['label'] == 'POSITIVE':\n",
        "            total_sentiment += 1\n",
        "        elif result[0]['label'] == 'NEGATIVE':\n",
        "            total_sentiment -= 1\n",
        "\n",
        "    # Καθορίζει το ολικό συναίσθημα με βάση το total_sentiment\n",
        "    if total_sentiment > 0:\n",
        "        return'POSITIVE'\n",
        "    elif total_sentiment < 0:\n",
        "        return 'NEGATIVE'\n",
        "    else:\n",
        "        return 'NEUTRAL'\n",
        "\n",
        "    return total_sentiment\n",
        "\n",
        "# Αφαιρούμε τα μη Αγγλικά σχόλια\n",
        "def is_english(text):\n",
        "    try:\n",
        "        return detect(str(text)) == 'en'\n",
        "    except LangDetectException:\n",
        "        return False\n",
        "\n",
        "portion23 = portion23[portion23['comments'].apply(is_english)]\n",
        "\n",
        "portion23['review'] = portion23['comments'].apply(clean_text)\n",
        "portion23['sentiment'] = portion23['review'].apply(classify_sentiment)\n",
        "\n",
        "portion23['review'] = portion23['review'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "portion23 = portion23.loc[:, ['listing_id', 'review', 'sentiment', 'month']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irayXyFDDfIk"
      },
      "source": [
        "**3ο bullet**\n",
        "\n",
        "Συγκρίνετε το συνολικό συναίσθημα με την πάροδο του χρόνου (πχ ένα\n",
        "ιστόγραμμα για κάθε χρόνο με την κατανομή των positive/negative/neutral)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ml9Z4_bTmg0"
      },
      "outputs": [],
      "source": [
        "pivot_sentiment19 = portion19.pivot_table(index='month', columns='sentiment', values='review', aggfunc='count', fill_value=0)\n",
        "\n",
        "# Φτιάχνουμε το γράφημα\n",
        "pivot_sentiment19.plot(kind='bar', stacked=True, figsize=(12, 6))\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of Comments')\n",
        "plt.title('Number of Positive, Negative, and Neutral Comments Per Year')\n",
        "plt.legend(title='Sentiment')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hn4E1rRJ5UG"
      },
      "outputs": [],
      "source": [
        "pivot_sentiment23 = portion23.pivot_table(index='month', columns='sentiment', values='review', aggfunc='count', fill_value=0)\n",
        "\n",
        "# Φτιάχνουμε το γράφημα\n",
        "pivot_sentiment23.plot(kind='bar', stacked=True, figsize=(12, 6))\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of Comments')\n",
        "plt.title('Number of Positive, Negative, and Neutral Comments Per Year')\n",
        "plt.legend(title='Sentiment')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoxEoxc9ZLKX"
      },
      "source": [
        "**BONUS**\n",
        "\n",
        "Συγκρίνετε το συναίσθημα ανά γειτονιά με την πάροδο του χρόνου"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqdQN8FbjnEo"
      },
      "outputs": [],
      "source": [
        "portion19['year'] = 2019\n",
        "portion23['year'] = 2023\n",
        "combined_df = pd.concat([portion19, portion23], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbXLcyy-m5mW"
      },
      "outputs": [],
      "source": [
        "# Διαβάζομυε τα listing.csv αρχεία για όλους τους μήνες\n",
        "listingFeb19 = pd.read_csv('/content/gdrive/MyDrive/data/data/2019/febrouary/listings.csv')\n",
        "listingMarch19 = pd.read_csv('/content/gdrive/MyDrive/data/data/2019/march/listings.csv')\n",
        "listingApril19 = pd.read_csv('/content/gdrive/MyDrive/data/data/2019/april/listings.csv')\n",
        "\n",
        "# Kαι προσθέτουμε σε μία καινούρια στήλη τον μήνα\n",
        "listingFeb19['month']='02'\n",
        "listingMarch19['month']='03'\n",
        "listingApril19['month'] = '04'\n",
        "\n",
        "# Ενώνουμε και τα 3 αρχεία σε ένα καινούριο\n",
        "train_2019 = pd.concat([listingFeb19, listingMarch19, listingApril19], ignore_index=False)\n",
        "train_2019.dropna()\n",
        "\n",
        "# Δημιουργούμε ενα αντίγραφο του train_2019 αλλά μόνο με μοναδικές τιμές με βάση το id\n",
        "train_2019_unique = train_2019.drop_duplicates(subset=['id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02rIydb7nIqo"
      },
      "outputs": [],
      "source": [
        "# Διαβάζομυε τα listing.csv αρχεία για όλους τους μήνες\n",
        "listingMarch23 = pd.read_csv('/content/gdrive/MyDrive/data/data/2023/march/listings.csv')\n",
        "listingJune23 = pd.read_csv('/content/gdrive/MyDrive/data/data/2023/june/listings.csv')\n",
        "listingSept23 = pd.read_csv('/content/gdrive/MyDrive/data/data/2023/september/listings.csv')\n",
        "\n",
        "# Kαι προσθέτουμε σε μία καινούρια στήλη τον μήνα\n",
        "listingMarch23['month'] = '03'\n",
        "listingJune23['month'] = '06'\n",
        "listingSept23['month'] = '09'\n",
        "\n",
        "# Ενώνουμε και τα 3 αρχεία σε ένα καινούριο\n",
        "train_2023 = pd.concat([listingMarch23, listingJune23, listingSept23], ignore_index=False)\n",
        "train_2023.dropna()\n",
        "train_2023.rename(columns={'useless': 'neighbourhood'}, inplace=True)\n",
        "train_2023.rename(columns={'neighbourhood': 'cleansed_neighbourhood'}, inplace=True)\n",
        "\n",
        "# Δημιουργούμε ενα αντίγραφο του train_2023 αλλά μόνο με μοναδικές τιμές με βάση το id\n",
        "train_2023_unique = train_2023.drop_duplicates(subset=['id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dN9mrx_dn8BU"
      },
      "outputs": [],
      "source": [
        "# Ενώνουμε τα 2 train αρχεία\n",
        "combined_train = pd.concat([train_2019_unique, train_2023_unique], ignore_index=True)\n",
        "\n",
        "# Μεταονομάζουμε την στήλη id σε listing_id\n",
        "combined_train.rename(columns={'id': 'listing_id'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4rH4ZNnoIHd"
      },
      "outputs": [],
      "source": [
        "# Συγχωνεύουμε τα αρχεία με το listing_id\n",
        "merged_dfs = pd.merge(combined_df, combined_train, on='listing_id', how='outer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "encTe0n0Va6D"
      },
      "outputs": [],
      "source": [
        "merged_dfs_per_neighbourhood_and_year = merged_dfs.groupby(['neighbourhood', 'year', 'sentiment']).size().unstack(fill_value=0)\n",
        "\n",
        "neighbourhoods = merged_dfs_per_neighbourhood_and_year.index.get_level_values('neighbourhood').unique()\n",
        "\n",
        "labels = ['POSITIVE', 'NEGATIVE', 'NEUTRAL']\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "color_mapping = {\n",
        "    (2019, 'POSITIVE'): 'green',\n",
        "    (2019, 'NEGATIVE'): 'blue',\n",
        "    (2019, 'NEUTRAL'): 'black',\n",
        "    (2023, 'NEGATIVE'): 'red',\n",
        "    (2023, 'POSITIVE'): 'orange',\n",
        "    (2023, 'NEUTRAL'): 'pink'\n",
        "}\n",
        "\n",
        "bar_width = 0.35\n",
        "years = [2019, 2023]\n",
        "\n",
        "for i, year in enumerate(years):\n",
        "    bar_positions = [pos + (i * bar_width) for pos in range(len(neighbourhoods))]\n",
        "\n",
        "    for j, label in enumerate(labels):\n",
        "        bar_heights = [merged_dfs_per_neighbourhood_and_year.loc[(neighbourhood, year), label] if (neighbourhood, year) in merged_dfs_per_neighbourhood_and_year.index else 0 for neighbourhood in neighbourhoods]\n",
        "        ax.bar(bar_positions, bar_heights, bar_width, label=f'{year} - {label}', color=color_mapping[(year, label)], edgecolor='white', alpha=0.7)\n",
        "\n",
        "# Προσαρμογή του άξονα x\n",
        "ax.set_xticks([pos + bar_width for pos in range(len(neighbourhoods))])\n",
        "ax.set_xticklabels(neighbourhoods, rotation=45, ha='right')\n",
        "\n",
        "# Προσθήκη ετικετών και τίτλου\n",
        "ax.set_xlabel('Neighbourhood')\n",
        "ax.set_ylabel('Number of Reviews')\n",
        "ax.set_title('Number of Reviews per Neighbourhood by Sentiment and Year')\n",
        "\n",
        "# Προσθήκη υπομνήματος\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp6_9ep4C4nu"
      },
      "source": [
        "# **ΕΡΏΤΗΜΑ 2ο**\n",
        "Στο ερώτημα αυτό θα χρειαστεί να δημιουργήσετε δύο νέα dataset:\n",
        "*  Ένα αρχείο train.tsv (θα είναι το 80% των συνολικών data points) που θα περιέχει τα δεδομένα που θα χρησιμοποιήσετε για εκπαίδευση των μοντέλων\n",
        "σας. Τα δεδομένα εκπαίδευσης περιέχουν την ένδειξη positive, negative ή\n",
        "neutral\n",
        "\n",
        "*  Ενα αρχείο test.tsv (το 20% των data points) το οποίο θα περιέχει τα δεδομένα\n",
        "που θα χρησιμοποιήσετε για να δοκιμάσετε το μοντέλο σας και να κάνετε μία\n",
        "πρόβλεψη. Πρέπει το μοντέλο σας να αποφασίσει για κάθε ένα από τα reviews\n",
        "που υπάρχουν στο σύνολο των test αν εφράζει θετικό, αρνητικό ή ουδέτερο\n",
        "συναίσθημα."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3WoYznfszKh"
      },
      "outputs": [],
      "source": [
        "train_data19, test_data19 = train_test_split(portion19, test_size=0.2, random_state=42)\n",
        "\n",
        "# Αποθήκευση των συνόλων σε αρχεία train.tsv και test.tsv\n",
        "train_data19.to_csv('train19.tsv', sep='\\t', index=False)\n",
        "test_data19.to_csv('test19.tsv', sep='\\t', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_Gr-wMkaymT"
      },
      "source": [
        "Η ίδια διαδικασία για το 2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40Il7soLa2hL"
      },
      "outputs": [],
      "source": [
        "train_data23, test_data23 = train_test_split(portion23, test_size=0.2, random_state=42)\n",
        "\n",
        "# Αποθήκευση των συνόλων σε αρχεία train.tsv και test.tsv\n",
        "train_data23.to_csv('train23.tsv', sep='\\t', index=False)\n",
        "test_data23.to_csv('test23.tsv', sep='\\t', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuxZrV20a70Z"
      },
      "source": [
        "Ακολουθήστε τις οδηγίες που παρουσιάσαμε στο φροντιστήριο και ετοιμάστε τα\n",
        "χαρακτηριστικά για κάθε review χρησιμοποιώντας:\n",
        "*   Tf-idf\n",
        "*   Word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koTH_LCBPX78"
      },
      "outputs": [],
      "source": [
        "v19 = TfidfVectorizer()\n",
        "train_tfidf19 = v19.fit_transform(train_data19['review'])\n",
        "test_tfidf19 = v19.transform(test_data19['review'])\n",
        "\n",
        "with open('tfidf_train19.pkl', 'wb') as f:\n",
        "    pickle.dump(train_tfidf19, f)\n",
        "with open('tfidf_test19.pkl', 'wb') as f:\n",
        "    pickle.dump(test_tfidf19, f)\n",
        "\n",
        "# Εκτυπώνουμε τον πίνακα TF-IDF για το train_data\n",
        "print(\"\\nTF-IDF Table for train data:\")\n",
        "print(train_tfidf19)\n",
        "\n",
        "# Εκτυπώνουμε τον πίνακα TF-IDF για το test_data\n",
        "print(\"\\nTF-IDF Table for test data:\")\n",
        "print(test_tfidf19)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5ZbYn2cQuJA"
      },
      "outputs": [],
      "source": [
        "# Word Embeddings χαρακτηριστικά (Word2Vec)\n",
        "# Εκπαίδευση του μοντέλου Word2Vec\n",
        "w2v_model19 = Word2Vec(sentences=train_data19['review'].apply(lambda x: x.split()), vector_size=100, window=5, min_count=1, workers=4)\n",
        "w2v_model19.save(\"word2vec19.model\")\n",
        "\n",
        "# Δημιουργία χαρακτηριστικών Word Embeddings\n",
        "def get_w2v_features(reviews, model, vector_size=100):\n",
        "    features = []\n",
        "    for review in reviews:\n",
        "        words = review.split()\n",
        "        word_vecs = [model.wv[word] for word in words if word in model.wv]\n",
        "        if len(word_vecs) == 0:\n",
        "            features.append(np.zeros(vector_size))\n",
        "        else:\n",
        "            features.append(np.mean(word_vecs, axis=0))\n",
        "    return np.array(features)\n",
        "\n",
        "train_w2v19 = get_w2v_features(train_data19['review'], w2v_model19)\n",
        "test_w2v19 = get_w2v_features(test_data19['review'], w2v_model19)\n",
        "\n",
        "# Αποθήκευση Word Embeddings χαρακτηριστικών\n",
        "with open('w2v_train19.pkl', 'wb') as f:\n",
        "    pickle.dump(train_w2v19, f)\n",
        "with open('w2v_test19.pkl', 'wb') as f:\n",
        "    pickle.dump(test_w2v19, f)\n",
        "\n",
        "# Εκτύπωση των embeddings για τα πρώτα 5 reviews στο training set\n",
        "print(\"First 5 training set embeddings:\")\n",
        "for i in range(5):\n",
        "    print(f\"Review {i+1} embedding:\", train_w2v19[i])\n",
        "\n",
        "# Εκτύπωση των embeddings για τα πρώτα 5 reviews στο test set\n",
        "print(\"\\nFirst 5 test set embeddings:\")\n",
        "for i in range(5):\n",
        "    print(f\"Review {i+1} embedding:\", test_w2v19[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5bpjn_ObkJv"
      },
      "source": [
        "Η ίδια διαδικασία για το 2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpMngbz4cAkX"
      },
      "outputs": [],
      "source": [
        "v23 = TfidfVectorizer()\n",
        "train_tfidf23 = v23.fit_transform(train_data23['review'])\n",
        "test_tfidf23 = v23.transform(test_data23['review'])\n",
        "\n",
        "with open('tfidf_train23.pkl', 'wb') as f:\n",
        "    pickle.dump(train_tfidf23, f)\n",
        "with open('tfidf_test23.pkl', 'wb') as f:\n",
        "    pickle.dump(test_tfidf23, f)\n",
        "\n",
        "# Εκτυπώνουμε τον πίνακα TF-IDF για το train_data\n",
        "print(\"\\nTF-IDF Table for train data:\")\n",
        "print(train_tfidf23)\n",
        "\n",
        "# Εκτυπώνουμε τον πίνακα TF-IDF για το test_data\n",
        "print(\"\\nTF-IDF Table for test data:\")\n",
        "print(test_tfidf23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A63kRXiwcFej"
      },
      "outputs": [],
      "source": [
        "# Word Embeddings χαρακτηριστικά (Word2Vec)\n",
        "# Εκπαίδευση του μοντέλου Word2Vec\n",
        "w2v_model23 = Word2Vec(sentences=train_data23['review'].apply(lambda x: x.split()), vector_size=100, window=5, min_count=1, workers=4)\n",
        "w2v_model23.save(\"word2vec23.model\")\n",
        "\n",
        "# Δημιουργία χαρακτηριστικών Word Embeddings\n",
        "def get_w2v_features(reviews, model, vector_size=100):\n",
        "    features = []\n",
        "    for review in reviews:\n",
        "        words = review.split()\n",
        "        word_vecs = [model.wv[word] for word in words if word in model.wv]\n",
        "        if len(word_vecs) == 0:\n",
        "            features.append(np.zeros(vector_size))\n",
        "        else:\n",
        "            features.append(np.mean(word_vecs, axis=0))\n",
        "    return np.array(features)\n",
        "\n",
        "train_w2v23 = get_w2v_features(train_data23['review'], w2v_model23)\n",
        "test_w2v23 = get_w2v_features(test_data23['review'], w2v_model23)\n",
        "\n",
        "# Αποθήκευση Word Embeddings χαρακτηριστικών\n",
        "with open('w2v_train23.pkl', 'wb') as f:\n",
        "    pickle.dump(train_w2v23, f)\n",
        "with open('w2v_test23.pkl', 'wb') as f:\n",
        "    pickle.dump(test_w2v23, f)\n",
        "\n",
        "# Εκτύπωση των embeddings για τα πρώτα 5 reviews στο training set\n",
        "print(\"First 5 training set embeddings:\")\n",
        "for i in range(5):\n",
        "    print(f\"Review {i+1} embedding:\", train_w2v23[i])\n",
        "\n",
        "# Εκτύπωση των embeddings για τα πρώτα 5 reviews στο test set\n",
        "print(\"\\nFirst 5 test set embeddings:\")\n",
        "for i in range(5):\n",
        "    print(f\"Review {i+1} embedding:\", test_w2v23[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ythKeoN-cJlg"
      },
      "source": [
        "Δοκιμάζουμε τους παρακάτω ταξινομητές (η δοκιμή/testing θα γίνει στα test\n",
        "δεδομένα που έχουμε κρατήσει- εννοείται ότι τα test δεν θα χρησιμοποιηθούν για το\n",
        "training!)\n",
        "\n",
        "*  SVM\n",
        "*  Random Forests\n",
        "*  KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj_yKLBQcRcg"
      },
      "source": [
        "Δοκιμάστε τους ταξινομητές σας με τα χαρακτηριστικά TFID, και word embeddings.\n",
        "Επίσης θα πρέπει να αξιολογήσετε και να καταγράψετε την απόδοση κάθε μεθόδου\n",
        "χρησιμοποιώντας 10-fold Cross Validation χρησιμοποιώντας τις παρακάτω μετρικές:\n",
        "\n",
        "*  Precision / Recall / F-Measure\n",
        "*  Accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Icgz5YHvRZNr"
      },
      "outputs": [],
      "source": [
        "# Φόρτωση χαρακτηριστικών και ετικετών\n",
        "with open('tfidf_train19.pkl', 'rb') as f:\n",
        "    train_tfidf19 = pickle.load(f)\n",
        "with open('tfidf_test19.pkl', 'rb') as f:\n",
        "    test_tfidf19 = pickle.load(f)\n",
        "with open('w2v_train19.pkl', 'rb') as f:\n",
        "    train_w2v19 = pickle.load(f)\n",
        "with open('w2v_test19.pkl', 'rb') as f:\n",
        "    test_w2v19 = pickle.load(f)\n",
        "\n",
        "y_train19 = train_data19['sentiment']\n",
        "y_test19 = test_data19['sentiment']\n",
        "\n",
        "# Ορισμός μοντέλων\n",
        "models = {\n",
        "    'SVM': SVC(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'KNN': KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "# Εκπαίδευση και αξιολόγηση των μοντέλων με TF-IDF\n",
        "for name, model in models.items():\n",
        "    model.fit(train_tfidf19, y_train19)\n",
        "    y_pred = model.predict(test_tfidf19)\n",
        "    accuracy = accuracy_score(y_test19, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test19, y_pred, average='weighted')\n",
        "    print(f'{name} with TF-IDF:')\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "    print(f'Precision: {precision}')\n",
        "    print(f'Recall: {recall}')\n",
        "    print(f'F1-Score: {f1}\\n')\n",
        "\n",
        "# Εκπαίδευση και αξιολόγηση των μοντέλων με Word Embeddings\n",
        "for name, model in models.items():\n",
        "    model.fit(train_w2v19, y_train19)\n",
        "    y_pred = model.predict(test_w2v19)\n",
        "    accuracy = accuracy_score(y_test19, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test19, y_pred, average='weighted')\n",
        "    print(f'{name} with Word Embeddings:')\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "    print(f'Precision: {precision}')\n",
        "    print(f'Recall: {recall}')\n",
        "    print(f'F1-Score: {f1}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V4wFCr1coN1"
      },
      "source": [
        "Η ίδια διαδικασία και για το 2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrcFhEC8co_B"
      },
      "outputs": [],
      "source": [
        "# Φόρτωση χαρακτηριστικών και ετικετών\n",
        "with open('tfidf_train23.pkl', 'rb') as f:\n",
        "    train_tfidf23 = pickle.load(f)\n",
        "with open('tfidf_test23.pkl', 'rb') as f:\n",
        "    test_tfidf23 = pickle.load(f)\n",
        "with open('w2v_train23.pkl', 'rb') as f:\n",
        "    train_w2v23 = pickle.load(f)\n",
        "with open('w2v_test23.pkl', 'rb') as f:\n",
        "    test_w2v23 = pickle.load(f)\n",
        "\n",
        "y_train23 = train_data23['sentiment']\n",
        "y_test23 = test_data23['sentiment']\n",
        "\n",
        "# Ορισμός μοντέλων\n",
        "models = {\n",
        "    'SVM': SVC(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'KNN': KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "# Εκπαίδευση και αξιολόγηση των μοντέλων με TF-IDF\n",
        "for name, model in models.items():\n",
        "    model.fit(train_tfidf23, y_train23)\n",
        "    y_pred = model.predict(test_tfidf23)\n",
        "    accuracy = accuracy_score(y_test23, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test23, y_pred, average='weighted')\n",
        "    print(f'{name} with TF-IDF:')\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "    print(f'Precision: {precision}')\n",
        "    print(f'Recall: {recall}')\n",
        "    print(f'F1-Score: {f1}\\n')\n",
        "\n",
        "# Εκπαίδευση και αξιολόγηση των μοντέλων με Word Embeddings\n",
        "for name, model in models.items():\n",
        "    model.fit(train_w2v23, y_train23)\n",
        "    y_pred = model.predict(test_w2v23)\n",
        "    accuracy = accuracy_score(y_test23, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test23, y_pred, average='weighted')\n",
        "    print(f'{name} with Word Embeddings:')\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "    print(f'Precision: {precision}')\n",
        "    print(f'Recall: {recall}')\n",
        "    print(f'F1-Score: {f1}\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTS9gQ74PDKZ"
      },
      "source": [
        "# **ΕΡΩΤΗΜΑ 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_4wWiE0DXVm"
      },
      "source": [
        "# **Ερώτημα 3**\n",
        "\n",
        "Για τη στήλη comments και με χρήση word\n",
        "embeddings φτιάξτε μία συνάρτηση η οποία θα\n",
        "υπολογίζει το similarity μεταξύ vectors δύο\n",
        "λέξεων (πχ cosine ή όποια άλλη μέθοδο\n",
        "θέλετε)\n",
        "Μπορείτε να χρησιμοποιήσετε οποιαδήποτε\n",
        "μέθοδο φτιάχνει word embeddings (word2vec,\n",
        "fastText, glove κτλ) η να χρησιμοποιήσετε έτοιμα pre-trained embeddings.\n",
        "Στη συνέχεια για δύο λέξεις (τυχαίες που θα δίνει ένας χρήστης) και για μία\n",
        "παράμετρο Ν να υπολογίζεται\n",
        "\n",
        "(α) η σημασιολογική γειτονιά των δύο λέξεων και\n",
        "\n",
        "(β)\n",
        "το similarity των λέξεων το οποίο θα προκύπτει από τη γειτονιά με βάση τα τρία\n",
        "παρακάτω σχήματα , δηλαδή 3 διαφορετικά similarities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LViKwynADQW9"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained Word2Vec embeddings\n",
        "word2vec_vectors = api.load(\"word2vec-google-news-300\")  # 300-dimensional Word2Vec embeddings\n",
        "\n",
        "def compute_similarity(word1, word2, model):\n",
        "    if word1 in model and word2 in model:\n",
        "        vec1 = model[word1].reshape(1, -1)\n",
        "        vec2 = model[word2].reshape(1, -1)\n",
        "        similarity = cosine_similarity(vec1, vec2)[0][0]\n",
        "        return similarity\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def neighborhood_similarity(word1, word2, model, N):\n",
        "\n",
        "    if word1 in model:\n",
        "      neighborhood1 = model.most_similar(word1, topn=N)\n",
        "    if word2 in model:\n",
        "      neighborhood2 = model.most_similar(word2, topn=N)\n",
        "\n",
        "    if not neighborhood1 or not neighborhood2:\n",
        "        return None\n",
        "\n",
        "    words1 = [w for w, _ in neighborhood1]\n",
        "    words2 = [w for w, _ in neighborhood2]\n",
        "\n",
        "    # Method 1: Average cosine similarity between all pairs of words in the neighborhoods\n",
        "    cosine_similarities = []\n",
        "    for w1 in words1:\n",
        "        for w2 in words2:\n",
        "            sim = compute_similarity(w1, w2, model)\n",
        "            if sim is not None:\n",
        "                cosine_similarities.append(sim)\n",
        "    avg_cosine_similarity = np.mean(cosine_similarities) if cosine_similarities else 0\n",
        "\n",
        "    # Method 2: Maximum cosine similarity between any pair of words in the neighborhoods\n",
        "    max_cosine_similarity = np.max(cosine_similarities) if cosine_similarities else 0\n",
        "\n",
        "    # Method 3: Minimum cosine similarity between any pair of words in the neighborhoods\n",
        "    min_cosine_similarity = np.min(cosine_similarities) if cosine_similarities else 0\n",
        "\n",
        "    return {\n",
        "        'avg_cosine_similarity': avg_cosine_similarity,\n",
        "        'max_cosine_similarity': max_cosine_similarity,\n",
        "        'min_cosine_similarity': min_cosine_similarity\n",
        "    }\n",
        "\n",
        "def analyze_similarity(word1, word2, model, N_values):\n",
        "    avg_similarities = []\n",
        "    max_similarities = []\n",
        "    min_similarities = []\n",
        "\n",
        "    for N in N_values:\n",
        "        similarities = neighborhood_similarity(word1, word2, model, N)\n",
        "        if similarities:\n",
        "            avg_similarities.append(similarities['avg_cosine_similarity'])\n",
        "            max_similarities.append(similarities['max_cosine_similarity'])\n",
        "            min_similarities.append(similarities['min_cosine_similarity'])\n",
        "        else:\n",
        "            avg_similarities.append(None)\n",
        "            max_similarities.append(None)\n",
        "            min_similarities.append(None)\n",
        "\n",
        "    return avg_similarities, max_similarities, min_similarities\n",
        "\n",
        "def display_neighborhoods(word, model, N):\n",
        "\n",
        "    if word in model:\n",
        "      neighborhood = model.most_similar(word, topn=N)\n",
        "\n",
        "    if neighborhood:\n",
        "        print(f\"Semantic Neighborhood for '{word}' (N={N}):\")\n",
        "        for neighbor, similarity in neighborhood:\n",
        "            print(f\"  {neighbor}: {similarity:.4f}\")\n",
        "    else:\n",
        "        print(f\"Word '{word}' not found in the model.\")\n",
        "\n",
        "# Define the words and N values\n",
        "word1 = \"Alex\"\n",
        "word2 = \"Anastasia\"\n",
        "N_values = [1, 5, 10, 20, 50, 100]\n",
        "\n",
        "avg_similarities, max_similarities, min_similarities = analyze_similarity(word1, word2, word2vec_vectors, N_values)\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(N_values, avg_similarities, label='Average Cosine Similarity', marker='o')\n",
        "plt.plot(N_values, max_similarities, label='Maximum Cosine Similarity', marker='o')\n",
        "plt.plot(N_values, min_similarities, label='Minimum Cosine Similarity', marker='o')\n",
        "plt.xlabel('N (Neighborhood Size)')\n",
        "plt.ylabel('Similarity')\n",
        "plt.title('Similarity vs Neighborhood Size')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Display neighborhoods for N = 10\n",
        "N = 10\n",
        "\n",
        "print(f\"Neighborhoods for N = {N}:\")\n",
        "display_neighborhoods(word1, word2vec_vectors, N)\n",
        "print()\n",
        "display_neighborhoods(word2, word2vec_vectors, N)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}